model:
  tokenizer_path: tokenizer_250k
  layer_norm_eps: 1.0e-05
  output_past: True
  type_vocab_size: 1
  max_length: 256
  hidden_size: 768
  num_attention_heads: 6
  num_hidden_layers: 10
  intermediate_size: 3072
  hidden_act: gelu
  position_embedding_type: absolute
  hidden_dropout_prob: 0.1
  initializer_range: 0.02

training:
  gradient_accumulation_steps: 2
  ignore_data_skip: False
  overwrite_output_dir: False
  seed: 1234
  max_steps: 500000
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  dataloader_num_workers: 6
  fp16: True
  save_steps: 100000
  save_total_limit: 1
  learning_rate: 0.001
  warmup_steps: 50000

data:
  train: data/train/
  eval:
    all: data/eval/all_eval.txt 
    per_lang: data/eval/