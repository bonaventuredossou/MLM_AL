model:
  tokenizer_path: tokenizer_250k
  layer_norm_eps: 1.0e-05
  output_past: True
  type_vocab_size: 1
  max_length: 256
  hidden_size: 768
  num_attention_heads: 6
  num_hidden_layers: 10
  intermediate_size: 3072
  hidden_act: gelu
  position_embedding_type: absolute
  hidden_dropout_prob: 0.1
  initializer_range: 0.02

training:
  gradient_accumulation_steps: 8
  ignore_data_skip: False
  overwrite_output_dir: False
  seed: 1234
  max_steps: 500000
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 32
  dataloader_num_workers: 6
  fp16: True
  save_steps: 200000
  save_total_limit: 1
  learning_rate: 0.0001
  warmup_steps: 40000